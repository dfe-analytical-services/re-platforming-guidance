# Case Studies{#case_studies}

:::: {.infoboxblue .info data-latex="info"}

The case studies included in this guide were identified through a cross-Department working group.  This covered analysts who were new to code-based models up to those who were already experts in this area. We hope that the case studies showcased will provide useful learning points for everybody, no matter where you are in your learning journey.  We hope to include additional case studies in future so please [get in touch](mailto:modellingandqa@Educationgovuk.onmicrosoft.com) if you would like to add yours!  

::::

In this chapter:

[Schools Block National Funding Formula Model (SB NFF)] : Re-platforming from Excel to R due to amount of data being processed.

[Student Loan Earnings Model] : A more technical case study looking at how re-platforming challenged the model's methodology.

## Schools Block National Funding Formula Model (SB NFF)
The SB NFF model carries out funding allocations for the schools block element of the Dedicated Schools Grant. Formulae within the model are underpinned by individual schools and pupil characteristics for more than 20,000 schools.

### Why should we re-platform?
Running the model in Excel involves several linked workbooks and data processing/manipulation in other files. The model often took most of a day for two analysts to update new changes for scenario costing. It was also very prone to error given the number of manual interventions required. The aim of the re-platform was to create a cleaner pipeline for scenario costing.

### What are the barriers to re-platforming?
One of the *biggest barriers* was that policy colleagues were initially reluctant for a re-platform as they wanted to be able to interact with the model themselves.

### Replatform details
There were two phases to the re-platform.

The first phase of the re-platform saw a single analyst create the initial model in R. This took a few months, alongside "Business as Usual" work. As this was a "third-build" model, the analyst was able to match outputs from the existing model to fully QA'ed models to ensure the R model was working correctly.

The second phase of the re-platform involved other members of the team implementing further QA and error checking into the model.  An important change that was needed at this stage was a restructure of the code to allow for an Rmarkdown document to be created for policy colleagues.  This was created iteratively, with a lot of engagement from working level policy colleagues in order to capture the right level of detail in an easy to understand way before taking more senior stakeholders and the model SRO through the walkthrough for sign-off.  After this, the R model became their main model for the following allocation cycle.  

### Benefits
* **Easier to update**
  + Updating the model parameters became easier on the new platform than in Excel.

* **More efficient**
  + The model is now much more efficient, running in seconds rather than tens of minutes.

* **Better QA**
  + The new model can query directly to SQL compared to the previous Excel model, which required generating/exporting code, then loading the data in. The SQL query is directly within the model and is therefore transparent, greatly enhancing QA. By querying the data directly, it removes risk of both copying and pasting errors or using the wrong file.

* **Improved knowledge transfer**
  + Other analysts in the team who took on the rest of the NFF re-development were totally new to R but had experience of other languages. They were able to use what had been done in the first phase to learn the language in a hands-on way by developing the next phase. The NFF team have also used their new R skills to produce other dashboards and tools using R and RShiny to support the policy team with correspondence, PMQs. etc

* **Better support to policy colleagues**
  + The re-platformed model meant the team were able to provide better support to the following year’s SR because it was much quicker and easier to run scenarios. Analysts were able to produce an output dashboard that included a number of different comparisons and breakdowns. This helped the policy team understand and communicate the impact of a particular funding scenario.

* **Better version control**
  + The team use repos within Azure DevOps and git for tracking changes and version control. This has improved governance of the model,  commissioning QA task processes and their ability to work on a single model collaboratively.

### Practical tips from the team

<p class="bubble thought">**Training** - On moving to R, team members had different levels of expertise, learning trajectories and amounts of time available to dedicate to on-the-job learning. There wasn’t a structured plan for what needed to be learnt, and training was very self-directed, which was manageable but tricky. Having an agreement in place for what fundamental learning needs to take place and allowing time for that training to be undertaken would be helpful for any re-platforming project.</p>

<p class="bubble thought">**Governance** - There were concerns from the policy team about whether they’d be able to understand the model and have enough confidence in it to sign it off. To overcome this, the analysts highlighted the benefits of re-platforming and provided walkthroughs of key chunks of the code. This took some effort from both sides – analysts needed to make sure code was clearly written and reasonably accessible, and the policy team needed to put in some effort to understand and follow a code-based model walkthrough. Thinking about how the re-platformed model would impact governance processes (e.g. dual builds, policy walkthroughs) and how to manage those changes was really important.</p>

<p class="bubble thought">**Resilience** - Whilst there is a move towards using R more widely in DfE, it’s still something many people are learning and levels of expertise vary.  This needs to be kept in mind when making developments.  The team need to consider whether they can expect someone else coming into the team to be able to pick up the modelling and how much training they will need. Potentially this has its advantages as there is a good amount of technical and non-technical development to be had from working in their team, but it’s something they need to bear in mind when making any substantive developments.</p>

## Student Loan Earnings Model
The [Student Loan Earnings Model](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/811624/Student_loan_forecasts_2018-19_-_technical_notes.pdf) is a micro-simulation model that forecasts future annual graduate earnings at an individual level, which are used to determine future student loan repayments that the Department expects to receive on its expenditure on student loans. The Earnings Model forecasts future annual earnings using a collection of submodels which: 

* For a given borrower's characteristics and history, predict next year’s employment status (from 4 possible states),
* If employed, forecast earnings using the suitable regression model.

The model is audited externally by the National Audit Office, who maintain a shadow-build version of the model. Whilst this is needed for NAO's own purposes, it adds value to the DfE process as well by acting as a dual build.  

It was re-platformed from Microsoft Excel to R over several phases, with the re-platform bringing out nuances in both the model methodology and implementation details. The process was carried out collaboratively by a small team, using version control software to track and merge changes.

### Why should we re-platform?
The Excel-based model implementation had several issues which made it complex to update and slow to run: 

* It required manual extraction of data stored externally in SQL databases - the Excel spreadsheet contained sample SQL code to extract the datasets, before the data was copied to an additional tab in the workbook to be used. 
* Key steps within the model's implementation logic were hidden within complex Excel formulae, and were time-consuming to trace and debug.
* The Excel model was only able to run a small sample (200,000) of the almost 5 million borrowers in the system, so it lacked capacity to model the entire population.
* An additional issue was that platform-specific issues in Excel (such as how random numbers are generated and stored) caused differences in reproducing simulation results on other platforms. 

The aim of the re-platform was to improve readability, reliability and speed of implementation, to permit larger model runs, and set up a robust version control methodology for future model improvement.

### Replatform details
The re-platform took place over phases by a small team of analysts - all quite new to the model but proficient in R and other coding languages.

They started by copying the workflow from the existing implementation^[This may have been done differently if re-platforming based directly on mathematical formulae rather than from an implementation.] - following the logic of calculation steps displayed within the cell formulae of the Excel spreadsheet, and incrementally ported each calculation step of the methodology across.

As part of the re-platform, lookup tables from Excel were exported as datasets but external SQL databases were queried directly from the re-platformed model.  This brought several benefits including ensuring changes to the code were only needed in one place, reducing the risk of copying and pasting errors by accessing the data directly and allowing the entire dataset to be used within the model, instead of only a small sample.

The team used version control within an Azure DevOps repo to carry out continuous code review during development. This meant that quality assurance occurred incrementally - potentially eliminating the need for a detailed 'full' quality assurance after the re-platform was completed.  However, wider [unit testing](https://dfe-analytical-services.github.io/good-code-practice/testing.html#unit-testing) may have been beneficial to provide assurance of the end to end process.

The re-platform was carried out prior to writing documentation. It was unclear to the team whether it was more advantageous to document along the way given the analyst time available. In hindsight this may have been a better option as it would mean analysts documenting each section they had replatformed.

### Benefits

* **Can utilise full dataset**
  + The model can utilise the full dataset and model outcomes for individual borrowers based on their full characteristics, rather than scaling results from a modelled subsample of aggregated borrowers.

* **Simpler model methodology**
  + The model methodology is easier to understand, with model parameters and subprocesses defined in a more readable way. This makes it easier to tweak the model for model runs, and simpler for new team members familiarising themselves with the model.

* **Quicker**  
  + Model run times are quicker, and it is easier for a single team member to execute multiple model runs.

* **Better version control**
  + A more robust version-control system is in place which means that model changes are easier to quality assure, model versions can be tracked and compared, and the model development process is more transparent for external audit.


### Technical tips
The team found that switching to a code-based implementation led to additional questions during the re-platform, which may recur in other projects. Some of these are listed below:

<link href='http://fonts.googleapis.com/css?family=Gloria+Hallelujah&amp;subset=latin' rel='stylesheet' type='text/css'>

<ul id="notes">
    <li>
        <p>Use appropriately named functions and variable names to simplify the readability</p>
    </li>
    <li>
        <p>Regular check-ins if multiple analysts working on code (code style/naming conventions)</p>
    </li>
    <li>
        <p>Camel case or snake case for functions, starting with a verb (e.g. calcNewBalance)</p>
    </li>
    <li>
        <p>Think about variable names - switching between SQL and R - upper case vs camel case/Pascal case</p>
    </li>
    <li>
        <p>Agree and decide on the optimum Git branching strategy - e.g. using merge vs rebase, fast forward history</p>
    </li>
    <li>
        <p>Don’t store large binary files in Git if you can help it!</p>
    </li>
    <li>
        <p>Consider using lists of lists to store the output data (memory issues)</p>
    </li>
    <li>
        <p>Consider using data.table() package over dplyr for speed with large datasets</p>
    </li>
</ul>